{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jVtVwXBoyYc"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pyspark import SparkConf,SparkContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Spark RDD Application\").config(conf = SparkConf()).getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import curve_fit, minimize\n",
        "import collections\n",
        "\n",
        "\n",
        "name_1 = [1, 4, 6, 9]\n",
        "\n",
        "name_2 = [0, 5, 7, 0]\n",
        "\n",
        "name_3 = [0, 0, 0, 0]\n",
        "\n",
        "name_4 = [-2, 3, 8, 1]\n",
        "\n",
        "df_data= pd.DataFrame({'name_1' : name_1 , 'name_2':name_2 , 'name_3':name_3 , 'name_4':name_4 })"
      ],
      "metadata": {
        "id": "1xKl-TpKo9jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "df = df_spark.select([(df_spark[column] - df_spark.agg(mean(df_spark[column]).alias(\"mean\")).collect()[0][\"mean\"] ).alias(column) for column in df_spark.columns])\n",
        "column_list = df.columns\n",
        "\n",
        "temp = []\n",
        "new_col = []\n",
        "for i,  col_name1 in enumerate(column_list):\n",
        "  temp.append(col_name1)\n",
        "  flag = 1\n",
        "  for j , col_name2 in enumerate(column_list):\n",
        "    if col_name2 not in temp:\n",
        "      a = str(i) +'_'+  str(j)\n",
        "      new_col.append(a)\n",
        "      df = df.withColumn(a , F.col(col_name1) * F.col(col_name2))\n",
        "df = df.select(new_col)\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col, sum\n",
        "df = df.select([sum(col(c)).alias(c) for c in df.columns])\n",
        "df = df.toPandas()\n",
        "\n",
        "mat = np.identity(4)\n",
        "\n",
        "index_name  = df.columns\n",
        "key_value = {}\n",
        "for i in index_name:\n",
        "  c = i.split('_')\n",
        "  mat[int(c[0]),int(c[1])] = float(df[i])\n",
        "  mat[int(c[1]),int(c[0])] = float(df[i])\n",
        "\n",
        "mat"
      ],
      "metadata": {
        "id": "Za9u4d8wo9g9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}